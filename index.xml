<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Solar Eye</title>
    <link>/</link>
    <description>Recent content on Deep Solar Eye</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 Mar 2018 11:54:10 +0530</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Dataset</title>
      <link>/posts/dataset/</link>
      <pubDate>Mon, 12 Mar 2018 11:54:10 +0530</pubDate>
      
      <guid>/posts/dataset/</guid>
      <description>We create a first-of-its-kind dataset, Solar-Panel-Soiling-Image dataset, comprising of 45,754 images of solar panels with power loss labels. Our experimental setup consists of two identical solar panels, which are kept side by side with an RGB camera facing them.

Soiling experiments were conducted on the first panel (close to the camera) while the other panel was used for reference. Images were captured at every 5 seconds and power generated by the panels was recorded. Soiling impact is reported as the percentage power loss with respect to the reference panel. Our data recording methodology was aimed to capture various types of soiling and their impact on PV panel.</description>
    </item>
    
    <item>
      <title>DeepSolarEye: A Deep Learning Based Solar Panel Visual Analytics</title>
      <link>/posts/blog/</link>
      <pubDate>Thu, 08 Mar 2018 23:10:22 +0530</pubDate>
      
      <guid>/posts/blog/</guid>
      <description>The impact of soiling on solar panels is an important and well-studied problem in renewable energy sector. In this project, we present the first convolutional neural network (CNN) based approach for solar panel soiling and defect analysis. Our approach takes an RGB image of solar panel and environmental factors as inputs to predict power loss, soiling localization, and soiling type. In computer vision, localization is a complex task which typically requires manually labeled training data such as bounding boxes or segmentation masks. Our proposed approach consists of specialized four stages which completely avoids localization ground truth and only needs panel images with power loss labels for training.</description>
    </item>
    
  </channel>
</rss>